{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\dev\\anaconda\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\dev\\anaconda\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\dev\\anaconda\\lib\\site-packages (from tensorflow) (3.2.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\dev\\anaconda\\lib\\site-packages (from tensorflow) (1.29.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\dev\\anaconda\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\dev\\anaconda\\lib\\site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\dev\\anaconda\\lib\\site-packages (from tensorflow) (3.12.2)\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in c:\\dev\\anaconda\\lib\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\dev\\anaconda\\lib\\site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in c:\\dev\\anaconda\\lib\\site-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in c:\\dev\\anaconda\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\dev\\anaconda\\lib\\site-packages (from tensorflow) (0.9.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\dev\\anaconda\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in c:\\dev\\anaconda\\lib\\site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\dev\\anaconda\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\dev\\anaconda\\lib\\site-packages (from tensorflow) (1.18.1)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in c:\\dev\\anaconda\\lib\\site-packages (from tensorflow) (1.4.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\dev\\anaconda\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: setuptools in c:\\dev\\anaconda\\lib\\site-packages (from protobuf>=3.8.0->tensorflow) (45.2.0.post20200210)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\dev\\anaconda\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\dev\\anaconda\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\dev\\anaconda\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\dev\\anaconda\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\dev\\anaconda\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.18.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\dev\\anaconda\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.0.post3)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\dev\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\dev\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\dev\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\dev\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\dev\\anaconda\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\dev\\anaconda\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.5.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\dev\\anaconda\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.1.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\dev\\anaconda\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in c:\\dev\\anaconda\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\dev\\anaconda\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\dev\\anaconda\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\dev\\anaconda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n",
      "Requirement already up-to-date: tensorflow-hub in c:\\dev\\anaconda\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in c:\\dev\\anaconda\\lib\\site-packages (from tensorflow-hub) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in c:\\dev\\anaconda\\lib\\site-packages (from tensorflow-hub) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in c:\\dev\\anaconda\\lib\\site-packages (from tensorflow-hub) (3.12.2)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\dev\\anaconda\\lib\\site-packages (from protobuf>=3.8.0->tensorflow-hub) (45.2.0.post20200210)\n",
      "Requirement already satisfied: bert-for-tf2 in c:\\dev\\anaconda\\lib\\site-packages (0.14.4)\n",
      "Requirement already satisfied: py-params>=0.9.6 in c:\\dev\\anaconda\\lib\\site-packages (from bert-for-tf2) (0.9.7)\n",
      "Requirement already satisfied: params-flow>=0.8.0 in c:\\dev\\anaconda\\lib\\site-packages (from bert-for-tf2) (0.8.2)\n",
      "Requirement already satisfied: tqdm in c:\\dev\\anaconda\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.42.1)\n",
      "Requirement already satisfied: numpy in c:\\dev\\anaconda\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\dev\\anaconda\\lib\\site-packages (0.1.91)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install --upgrade tensorflow-hub\n",
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('./DATA/train.csv', index_col='id')\n",
    "test=pd.read_csv('./DATA/test.csv', index_col='id') # final test dataset for the kaggle competition\n",
    "submission = pd.read_csv(\"./DATA/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10869</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10870</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      keyword location                                               text  \\\n",
       "id                                                                          \n",
       "1         NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "4         NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "5         NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "6         NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "7         NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "...       ...      ...                                                ...   \n",
       "10869     NaN      NaN  Two giant cranes holding a bridge collapse int...   \n",
       "10870     NaN      NaN  @aria_ahrary @TheTawniest The out of control w...   \n",
       "10871     NaN      NaN  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   \n",
       "10872     NaN      NaN  Police investigating after an e-bike collided ...   \n",
       "10873     NaN      NaN  The Latest: More Homes Razed by Northern Calif...   \n",
       "\n",
       "       target  \n",
       "id             \n",
       "1           1  \n",
       "4           1  \n",
       "5           1  \n",
       "6           1  \n",
       "7           1  \n",
       "...       ...  \n",
       "10869       1  \n",
       "10870       1  \n",
       "10871       1  \n",
       "10872       1  \n",
       "10873       1  \n",
       "\n",
       "[7613 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.2 % of training data have keyword, 0.8 % of values are missing\n",
      "99.2 % of training data have location, 0.8 % of values are missing\n",
      "99.2 % of test data have keyword, 0.8 % of values are missing\n",
      "66.1 % of test data have location, 33.9 % of values are missing\n"
     ]
    }
   ],
   "source": [
    "training_has_keyword = train[\"keyword\"].notna().value_counts(normalize=True)\n",
    "training_has_location = train[\"location\"].notna().value_counts(normalize=True)\n",
    "try:\n",
    "    print(f'{training_has_keyword[True]*100:.1f}', \"% of training data have keyword,\", f'{(training_has_keyword[False])*100:.1f}', \"% of values are missing\")\n",
    "    print(f'{training_has_keyword[True]*100:.1f}', \"% of training data have location,\", f'{(training_has_keyword[False])*100:.1f}', \"% of values are missing\")\n",
    "\n",
    "    test_has_keyword = test[\"keyword\"].notna().value_counts(normalize=True)\n",
    "    test_has_location = test[\"location\"].notna().value_counts(normalize=True)\n",
    "    print(f'{test_has_keyword[True]*100:.1f}', \"% of test data have keyword,\", f'{(test_has_keyword[False])*100:.1f}', \"% of values are missing\")\n",
    "    print(f'{test_has_location[True]*100:.1f}', \"% of test data have location,\", f'{(test_has_location[False])*100:.1f}', \"% of values are missing\")\n",
    "except KeyError:\n",
    "    print(\"NaN values already replaced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing NaN with missing_keyword and missing_location\n"
     ]
    }
   ],
   "source": [
    "print(\"Replacing NaN with missing_keyword and missing_location\")\n",
    "train[\"keyword\"] = train[\"keyword\"].fillna('missing_keyword')\n",
    "train[\"location\"] = train[\"location\"].fillna('missing_location')\n",
    "test[\"keyword\"] = test[\"keyword\"].fillna('missing_keyword')\n",
    "test[\"location\"] = test[\"location\"].fillna('missing_location')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates = train.groupby(['text']).count().sort_values(by='target', ascending=False)\n",
    "duplicates = duplicates[duplicates['target']>1]\n",
    "duplicates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 7613 tweets with 222 unique keywords.\n",
      "We have 7613 tweets with 3342 unique locations.\n"
     ]
    }
   ],
   "source": [
    "print(\"We have\", len(train[\"keyword\"]), \"tweets with\", train[\"keyword\"].nunique(), \"unique keywords.\")\n",
    "print(\"We have\", len(train[\"location\"]), \"tweets with\", train[\"location\"].nunique(), \"unique locations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location\n",
       "missing_location    1075\n",
       "USA                   67\n",
       "United States         27\n",
       "Nigeria               22\n",
       "India                 20\n",
       "Mumbai                19\n",
       "UK                    16\n",
       "New York              16\n",
       "London                16\n",
       "Washington, DC        15\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = train.groupby(train[\"keyword\"])\n",
    "grouped.size().sort_values()\n",
    "\n",
    "grouped = train.groupby(train[\"location\"])\n",
    "grouped.target.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_parameters = {\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(tweet_texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "\n",
    "    for text in tweet_texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "FullTokenizer=bert.bert_tokenization.FullTokenizer\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = bert_encode(train.text.values, tokenizer, max_len=160)\n",
    "test_input = bert_encode(test.text.values, tokenizer, max_len=160)\n",
    "train_labels = train.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ff41b1efc7e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbert_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m160\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model.h5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'build_model' is not defined"
     ]
    }
   ],
   "source": [
    "model = build_model(bert_layer, max_len=160)\n",
    "model.summary()\n",
    "\n",
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_2 (KerasLayer)      [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 1024)]       0           keras_layer_2[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        tf_op_layer_strided_slice[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 335,142,914\n",
      "Trainable params: 335,142,913\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/3\n",
      " 14/381 [>.............................] - ETA: 1:54:07 - loss: 0.6768 - accuracy: 0.5848"
     ]
    }
   ],
   "source": [
    "train_history = model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=3,\n",
    "    callbacks=[checkpoint],\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.h5')\n",
    "test_pred = model.predict(test_input)\n",
    "\n",
    "submission['target'] = test_pred.round().astype(int)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_tweet(tweet)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_disaster_tweets(train):\n",
    "    train_input = bert_encode(train.text.values, tokenizer, max_len=160)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = bert_encode(train.text.values, tokenizer, max_len=160)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
