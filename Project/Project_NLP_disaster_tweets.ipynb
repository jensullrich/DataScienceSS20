{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Kopie von Kopie von Project_NLP_disaster_tweets.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "oSgxmRPFcrKC",
        "0uXaQe_5cktZ",
        "-aHFt7Ovhivd",
        "o5TF59VBhxu6"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jensullrich/DataScienceSS20/blob/Project/Project/Project_NLP_disaster_tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3VqmBdVjTha",
        "colab_type": "text"
      },
      "source": [
        "# 1. Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF3ebFyhjl-v",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilDCD8aJR0oD",
        "colab_type": "text"
      },
      "source": [
        "Zuerst werden einige Pakete installiert, die in Colab nicht standardmäßig vorhanden sind. Alles weitere sollte vorinstalliert sein. \n",
        "\n",
        "Als Framework wird Tensorflow in der Version 2.3 verwendet. \n",
        "\n",
        "Als Architektur wird die, in der Vorlesung vorgestellte, Architektur BERT verwendet. Es wird die fertige Implementierung bert-for-tf2 in der aktuellen Version 0.14.5 genutzt. \n",
        "Quelle: https://pypi.org/project/bert-for-tf2/\n",
        "\n",
        "Vortrainierte Modelle werden von Tensorflow-Hub heruntergeladen. \n",
        "Quelle: https://tfhub.dev/s?q=bert\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhWwrTl9WhtZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "82cb552b-e5a9-41ea-aafb-9480178bfde6"
      },
      "source": [
        "!pip install tensorflow\n",
        "!pip install --upgrade tensorflow-hub\n",
        "!pip install bert-for-tf2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.30.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (49.2.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already up-to-date: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (49.2.0)\n",
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.14.5)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.9.7)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EDLMQIpjroy",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca78jJXhUWhI",
        "colab_type": "text"
      },
      "source": [
        "Hier befinden sich gesammelt alle Imports des Projekts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXBtB1X_Whtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input, Concatenate, Dropout, Average\n",
        "from tensorflow.keras.optimizers import Optimizer, Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow_hub as hub\n",
        "import bert\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLyTrMubj0jZ",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRzua5M0UeGF",
        "colab_type": "text"
      },
      "source": [
        "Das Projekt ist in Google Colab entwickelt und getestet worden, sollte aber auch lokal funktionieren, wenn eine passende GPU vorhanden ist."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQx9fefhW1DH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cb7f4f34-2fb2-4576-d59e-ebbeb0b22c1c"
      },
      "source": [
        "#check if notebook runs in colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print('running in Colab:',IN_COLAB)\n",
        "path='..'\n",
        "if IN_COLAB:\n",
        "  #in colab, we need to clone the data from the repo\n",
        "  !git clone -b Project https://github.com/jensullrich/DataScienceSS20/\n",
        "  path='DataScienceSS20/Project'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running in Colab: True\n",
            "fatal: destination path 'DataScienceSS20' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogJoYcz5U8Al",
        "colab_type": "text"
      },
      "source": [
        "Da das BERT Modell sehr groß ist, wird sehr viel GPU-Speicher benötigt. Deshalb wird hier am Anfang geprüft ob eine GPU mit ausreichend Speicher zugeteilt wurde. Es sollten 16GB sein, ist das nicht der Fall sollte die Runtime resettet werden, bis eine passende GPU zur Verfügung steht."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFpC-TWQlIhr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "0b8ab24e-3e9a-46cb-832d-7c3b8e35ddcd"
      },
      "source": [
        "#check out the GPU\n",
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Aug  9 10:06:57 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrwkowLQWhtY",
        "colab_type": "text"
      },
      "source": [
        "# 2. Exploring Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7SK7tc-VgdK",
        "colab_type": "text"
      },
      "source": [
        "Die Daten werden zunächst explorativ untersucht. Das Traininsset besteht aus 7613 Tweets, die mit 0 oder 1 für \"No Disaster\" oder \"Disaster\" gelabelt sind. Zusätzlich gibt es bei einigen Tweets noch die Location und ein Keyword. Diese können aber auch fehlen.\n",
        "\n",
        "Das Testset enthält 3263 Tweets ohne Label. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf7DIuDaWhtf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read the given csv files\n",
        "train=pd.read_csv(path + '/DATA/train.csv', index_col='id')\n",
        "test=pd.read_csv(path + '/DATA/test.csv', index_col='id')\n",
        "submission = pd.read_csv(path + '/DATA/sample_submission.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXV_wFGLWhti",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "5e5cadc8-db14-4463-cf6a-eb3d2c559deb"
      },
      "source": [
        "print(train.shape)\n",
        "print(test.shape)\n",
        "\n",
        "print(train.head())\n",
        "print(test.head())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7613, 4)\n",
            "(3263, 3)\n",
            "   keyword location                                               text  target\n",
            "id                                                                            \n",
            "1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1\n",
            "4      NaN      NaN             Forest fire near La Ronge Sask. Canada       1\n",
            "5      NaN      NaN  All residents asked to 'shelter in place' are ...       1\n",
            "6      NaN      NaN  13,000 people receive #wildfires evacuation or...       1\n",
            "7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...       1\n",
            "   keyword location                                               text\n",
            "id                                                                    \n",
            "0      NaN      NaN                 Just happened a terrible car crash\n",
            "2      NaN      NaN  Heard about #earthquake is different cities, s...\n",
            "3      NaN      NaN  there is a forest fire at spot pond, geese are...\n",
            "9      NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
            "11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-LasmpKWhtk",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 NaN Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em3DRgAKmK-l",
        "colab_type": "text"
      },
      "source": [
        "Laut Beschreibung können sowohl Keyword, als auch Location fehlen. Es wird also zunächst untersucht wie viele Werte fehlen. Beim Keyword ist der Anteil an fehlenden Werten sehr gering, was für eine Verwendung spricht. Bei rund einem Drittel der Daten fehlt die Location, diese ist somit eher schlecht als Feature geeignet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CO1zyVqWhtl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "92629be8-9ff3-4756-dd3a-f1ba0aa408fa"
      },
      "source": [
        "training_has_keyword = train[\"keyword\"].notna().value_counts(normalize=True)\n",
        "training_has_location = train[\"location\"].notna().value_counts(normalize=True)\n",
        "test_has_keyword = test[\"keyword\"].notna().value_counts(normalize=True)\n",
        "test_has_location = test[\"location\"].notna().value_counts(normalize=True)\n",
        "\n",
        "try:\n",
        "    print(f'{training_has_keyword[True]*100:.1f}', \"% of training data have keyword,\", f'{(training_has_keyword[False])*100:.1f}', \"% of keywords are missing\")\n",
        "    print(f'{training_has_location[True]*100:.1f}', \"% of training data have location,\", f'{(training_has_location[False])*100:.1f}', \"% of locations are missing\")\n",
        "    print(f'{test_has_keyword[True]*100:.1f}', \"% of test data have keyword,\", f'{(test_has_keyword[False])*100:.1f}', \"% of keywords are missing\")\n",
        "    print(f'{test_has_location[True]*100:.1f}', \"% of test data have location,\", f'{(test_has_location[False])*100:.1f}', \"% of locations are missing\")\n",
        "except KeyError:\n",
        "    print(\"No NaN values or they have already been replaced\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99.2 % of training data have keyword, 0.8 % of keywords are missing\n",
            "66.7 % of training data have location, 33.3 % of locations are missing\n",
            "99.2 % of test data have keyword, 0.8 % of keywords are missing\n",
            "66.1 % of test data have location, 33.9 % of locations are missing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHZ6Ef6Fm2f4",
        "colab_type": "text"
      },
      "source": [
        "Die fehlenden Werte bekommen die aussagekräftigeren Einträge 'missing_keyword' bzw. 'missing_location'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CELx-VjZWhtn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "03c26219-f3d3-41ff-9628-4bfb61325fea"
      },
      "source": [
        "print(\"Replacing NaN values with missing_keyword and missing_location\")\n",
        "train[\"keyword\"] = train[\"keyword\"].fillna('missing_keyword')\n",
        "train[\"location\"] = train[\"location\"].fillna('missing_location')\n",
        "test[\"keyword\"] = test[\"keyword\"].fillna('missing_keyword')\n",
        "test[\"location\"] = test[\"location\"].fillna('missing_location')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Replacing NaN values with missing_keyword and missing_location\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZJpW-e0Whtp",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Duplicate Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ze39Vt1Xvuh",
        "colab_type": "text"
      },
      "source": [
        "Im Trainingsdatensatz kommen einige Tweets mehrfach vor. Bei Zeilen die komplett doppelt vorkommen, inklusive Target, Keyword und Location, werden nur die Duplikate gelöscht. Bei Zeilen, wo nur der Text übereinstimmt, werden alle gelöscht, da vor allem ein verschiedenes Target das Modell stören könnte. Insgesamt werden so 158 Zeilen gelöscht.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t60FUDpWhtq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e346f31d-ace6-4d18-e968-9af5fc78f993"
      },
      "source": [
        "# remove complete duplicates, keep only the first occurence\n",
        "print(train.shape)\n",
        "train.drop_duplicates(keep='first', inplace=True)\n",
        "print(train.shape)\n",
        "# check other duplicates\n",
        "train.drop_duplicates('text', keep=False, inplace=True)\n",
        "print(train.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7613, 4)\n",
            "(7561, 4)\n",
            "(7455, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GViHqonsWhts",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 Exploration of keyword and location"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyXyBInvZ3s5",
        "colab_type": "text"
      },
      "source": [
        "Als nächstes werden die Zusatzfeatures Keyword und Location nochmals untersucht. Es gibt 221 verschiedene Keywords und 3306 verschiedene Locations. Wie schon bei den fehlenden Werten stellt sich auch hier heraus, dass das Keyword durchaus hilfreich sein könnte um die Tweets zu klassifizieren. Die Location ist eher ungeeignet, da es zu viele verschiedene Werte gibt, jeder Wert nur wenige Male vorkommt und zusätzlich viele Werte fehlen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTIIdk0hWhtt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8699cc35-d932-40de-8ca8-aec9338c36d2"
      },
      "source": [
        "print(\"We have\", len(train[\"keyword\"]), \"tweets with\", train[\"keyword\"].nunique(), \"unique keywords.\")\n",
        "print(\"We have\", len(train[\"location\"]), \"tweets with\", train[\"location\"].nunique(), \"unique locations.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 7455 tweets with 222 unique keywords.\n",
            "We have 7455 tweets with 3307 unique locations.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M2Bi9ZuWhtv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "8b4ade21-538a-46f7-9594-e713a2994c55"
      },
      "source": [
        "grouped = train.groupby(train[\"keyword\"])\n",
        "grouped.size().sort_values()\n",
        "\n",
        "grouped = train.groupby(train[\"location\"])\n",
        "grouped.target.sum().sort_values(ascending=False).head(10)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "location\n",
              "missing_location    1030\n",
              "USA                   67\n",
              "United States         27\n",
              "Mumbai                18\n",
              "Nigeria               17\n",
              "India                 17\n",
              "New York              16\n",
              "London                16\n",
              "Washington, DC        15\n",
              "UK                    15\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzaVzysnoKXS",
        "colab_type": "text"
      },
      "source": [
        "## 2.4 Meta Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RprbhamDazf3",
        "colab_type": "text"
      },
      "source": [
        "Aus dem Text des Tweets lassen sich noch einige Features bestimmen, die eventuell nützlich sein könnten.\n",
        "\n",
        "\n",
        "*   Anzahl der Wörter\n",
        "*   Anzahl der verschiedenen Wörter\n",
        "*   Anzahl an URLs\n",
        "*   Durchschnittliche länge der Wörter\n",
        "*   Anzahl der Buchstaben\n",
        "*   Anzahl der Hashtags\n",
        "*   Anzahl der Erwähnungen mit @\n",
        "*   keyword_target_mean ist der durchschnittliche Targetwert, bei Tweets mit diesem Keyword\n",
        "\n",
        "Die zusätzlichen Input-Daten werden dann skaliert und als meta_input für das Modell vorbereitet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBeJby5fqMmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word_count\n",
        "train['word_count'] = train['text'].apply(lambda x: len(str(x).split()))\n",
        "test['word_count'] = test['text'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# unique_word_count\n",
        "train['unique_word_count'] = train['text'].apply(lambda x: len(set(str(x).split())))\n",
        "test['unique_word_count'] = test['text'].apply(lambda x: len(set(str(x).split())))\n",
        "\n",
        "# url_count\n",
        "train['url_count'] = train['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
        "test['url_count'] = test['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
        "\n",
        "# mean_word_length\n",
        "train['mean_word_length'] = train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "test['mean_word_length'] = test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "\n",
        "# char_count\n",
        "train['char_count'] = train['text'].apply(lambda x: len(str(x)))\n",
        "test['char_count'] = test['text'].apply(lambda x: len(str(x)))\n",
        "\n",
        "# hashtag_count\n",
        "train['hashtag_count'] = train['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
        "test['hashtag_count'] = test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
        "\n",
        "# mention_count\n",
        "train['mention_count'] = train['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
        "test['mention_count'] = test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
        "\n",
        "# keyword target mean\n",
        "train['keyword_target_mean'] = train.groupby('keyword')['target'].transform('mean')\n",
        "test['keyword_target_mean'] = test['keyword'].apply(lambda x: train.loc[train['keyword'] == x, 'keyword_target_mean'].iloc[0])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPRhgrTmWhtx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "630e8de4-5506-45d1-a656-1ca75f49346c"
      },
      "source": [
        "#define and scale our added features\n",
        "train_meta_input = StandardScaler().fit_transform(train.iloc[:, 4:])\n",
        "test_meta_input = StandardScaler().fit_transform(test.iloc[:, 3:])\n",
        "\n",
        "META_DIM = train_meta_input.shape[1]\n",
        "print(META_DIM)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF9rT9cyoP6V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "13187dbe-dbe3-499e-e2d3-73a6429e7675"
      },
      "source": [
        "train.sort_values(by='keyword_target_mean', ascending=True).head(100)\n",
        "#test.sort_values(by='keyword_target_mean', ascending=False).head(10)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>word_count</th>\n",
              "      <th>unique_word_count</th>\n",
              "      <th>url_count</th>\n",
              "      <th>mean_word_length</th>\n",
              "      <th>char_count</th>\n",
              "      <th>hashtag_count</th>\n",
              "      <th>mention_count</th>\n",
              "      <th>keyword_target_mean</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>aftershock</td>\n",
              "      <td>United States</td>\n",
              "      <td>&amp;gt;&amp;gt; $15 Aftershock : Protect Yourself and...</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>7.187500</td>\n",
              "      <td>130</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>aftershock</td>\n",
              "      <td>Instagram - @heyimginog</td>\n",
              "      <td>@afterShock_DeLo scuf ps live and the game... cya</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>5.250000</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>aftershock</td>\n",
              "      <td>304</td>\n",
              "      <td>'The man who can drive himself further once th...</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>4.500000</td>\n",
              "      <td>110</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>aftershock</td>\n",
              "      <td>Switzerland</td>\n",
              "      <td>320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/yN...</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>7.687500</td>\n",
              "      <td>138</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>aftershock</td>\n",
              "      <td>304</td>\n",
              "      <td>'There is no victory at bargain basement price...</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>5.727273</td>\n",
              "      <td>73</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8356</th>\n",
              "      <td>ruin</td>\n",
              "      <td>Garrett</td>\n",
              "      <td>like why on earth would you want anybody to be...</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>101</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.027027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8375</th>\n",
              "      <td>ruin</td>\n",
              "      <td>Winnipeg, Manitoba</td>\n",
              "      <td>Why do u ruin everything?  @9tarbox u ruined t...</td>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>4.190476</td>\n",
              "      <td>109</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.027027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8382</th>\n",
              "      <td>ruin</td>\n",
              "      <td>Florida Forever</td>\n",
              "      <td>RT to ruin @connormidd 's day.  http://t.co/kr...</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6.714286</td>\n",
              "      <td>54</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.027027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8385</th>\n",
              "      <td>ruin</td>\n",
              "      <td>#RedSoxNation</td>\n",
              "      <td>@JulieChen she shouldn't. Being with them is g...</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>4.588235</td>\n",
              "      <td>94</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.027027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8378</th>\n",
              "      <td>ruin</td>\n",
              "      <td>Boston</td>\n",
              "      <td>Lol The real issue is the the way the NFL is t...</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>4.722222</td>\n",
              "      <td>102</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.027027</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         keyword                  location  ... mention_count  keyword_target_mean\n",
              "id                                          ...                                   \n",
              "178   aftershock             United States  ...             1             0.000000\n",
              "146   aftershock  Instagram - @heyimginog   ...             1             0.000000\n",
              "149   aftershock                       304  ...             0             0.000000\n",
              "151   aftershock               Switzerland  ...             1             0.000000\n",
              "153   aftershock                       304  ...             0             0.000000\n",
              "...          ...                       ...  ...           ...                  ...\n",
              "8356        ruin                   Garrett  ...             0             0.027027\n",
              "8375        ruin        Winnipeg, Manitoba  ...             1             0.027027\n",
              "8382        ruin           Florida Forever  ...             1             0.027027\n",
              "8385        ruin             #RedSoxNation  ...             1             0.027027\n",
              "8378        ruin                    Boston  ...             0             0.027027\n",
              "\n",
              "[100 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m6Dc-L7h4Fk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "9aee5de8-b700-42ea-8072-9f01da0c4fdf"
      },
      "source": [
        "print(train['char_count'].max())\n",
        "plt.hist(train['char_count'], bins = 30) \n",
        "plt.title(\"histogram\") \n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjMM1Fn2cyG2",
        "colab_type": "text"
      },
      "source": [
        "Das Histogramm zeigt, dass die meisten Tweets zwischen 130 und 140 Character lang sind. Dies ist wichtig zu wissen, um später die max_seq_length für BERT festzulegen. Die Länge wirkt sich direkt auf den GPU-Speicherbedarf aus und sollte deshalb so niedrig wie möglich sein. Eine Länge von 160 könnte alle Tweets komplett erfassen, um zu sparen kann man bis auf 140 herunter gehen ohne zu viele Tweets zu beschneiden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf7mcleFWhtz",
        "colab_type": "text"
      },
      "source": [
        "# 3. Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfj8Sh5aem2L",
        "colab_type": "text"
      },
      "source": [
        "Die folgende Funktion kodiert die tweets, sodass das vortrainierte BERT-Modell damit arbeiten kann.\n",
        "Quelle: https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5N0HPbIWht2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bert_encode(tweet_texts, tokenizer, max_len=512):\n",
        "    all_tokens = []\n",
        "    all_masks = []\n",
        "    all_segments = []\n",
        "\n",
        "    for text in tweet_texts:\n",
        "        text = tokenizer.tokenize(text)\n",
        "            \n",
        "        text = text[:max_len-2]\n",
        "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "        pad_len = max_len - len(input_sequence)\n",
        "        \n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
        "        tokens += [0] * pad_len\n",
        "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "        segment_ids = [0] * max_len\n",
        "        \n",
        "        all_tokens.append(tokens)\n",
        "        all_masks.append(pad_masks)\n",
        "        all_segments.append(segment_ids)\n",
        "    \n",
        "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhhdWM-ae4Bv",
        "colab_type": "text"
      },
      "source": [
        "*   Beim Modell wird ein zusätzliches Layer für die Meta-Infos hinzugefügt.\n",
        "*   Außerdem wird mixed_precision beim Optimizer aktiviert um Speicher zu sparen.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkGvFVzfWht4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(bert_layer, max_len=512):\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "    meta_input = Input(shape=(META_DIM,), dtype=tf.float32, name='meta_input')\n",
        "\n",
        "    pool_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "    x = Dense(32,activation='relu')(pool_output)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Concatenate()([x, meta_input])\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    out = Dense(1, activation='sigmoid')(x)\n",
        "    \n",
        "    model = Model(inputs=[input_word_ids, input_mask, segment_ids, meta_input], outputs=out)\n",
        "    opt = tf.keras.optimizers.Adam(lr=1e-5)\n",
        "    opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\n",
        "    model.compile(opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJkWriMng1CW",
        "colab_type": "text"
      },
      "source": [
        "Es werden vortrainierte BERT-Layer verwendet. Diese gibt es in der Base-Version mit ca. 110 Mio lernbaren Parametern und in der Large-Version mit ca. 335 Mio lernbaren Parametern. Die Layer werden von Tensorflow-Hub heruntergeladen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSgxmRPFcrKC",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 Build Base Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_mY6f-oWht6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b58e5f02-de32-47fd-c6a1-806bde5eda77"
      },
      "source": [
        "%%time\n",
        "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\"\n",
        "bert_layer = hub.KerasLayer(module_url, trainable=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4.31 s, sys: 948 ms, total: 5.26 s\n",
            "Wall time: 5.34 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eD7fFqLWht8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "FullTokenizer=bert.bert_tokenization.FullTokenizer\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pz0tDqIcWht_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "cfbbac2e-1d06-415b-ba10-f0136bd1f899"
      },
      "source": [
        "model = build_model(bert_layer, max_len=150)\n",
        "model.summary()\n",
        "\n",
        "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 150)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 150)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 150)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 32)           24608       keras_layer[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 32)           0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "meta_input (InputLayer)         [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 40)           0           dropout[0][0]                    \n",
            "                                                                 meta_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 40)           0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            41          dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 109,506,890\n",
            "Trainable params: 109,506,889\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uXaQe_5cktZ",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 Build Large Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCxBp96Mc3CH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e9f15172-f521-4164-c5b0-3c2d3cb5bd1b"
      },
      "source": [
        "%%time\n",
        "module_large_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/2\"\n",
        "bert_large_layer = hub.KerasLayer(module_large_url, trainable=True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 7.73 s, sys: 1.54 s, total: 9.27 s\n",
            "Wall time: 9.13 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDGD_49bdC4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_file = bert_large_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_large_layer.resolved_object.do_lower_case.numpy()\n",
        "FullTokenizer=bert.bert_tokenization.FullTokenizer\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du5IjxoldZlp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "b34ea1b1-a56e-4f43-a2e7-3e71bddfc72c"
      },
      "source": [
        "model_large = build_model(bert_large_layer, max_len=150)\n",
        "model_large.summary()\n",
        "checkpoint = ModelCheckpoint('model_large.h5', monitor='val_loss', save_best_only=True)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 150)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 150)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 150)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer_1 (KerasLayer)      [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 32)           32800       keras_layer_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 32)           0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "meta_input (InputLayer)         [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 40)           0           dropout_2[0][0]                  \n",
            "                                                                 meta_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 40)           0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            41          dropout_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 335,174,730\n",
            "Trainable params: 335,174,729\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP_hxt4Yxc2u",
        "colab_type": "text"
      },
      "source": [
        "# 4. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AXL5eTDWht-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_input = bert_encode(train.text.values, tokenizer, max_len=150)\n",
        "test_input = bert_encode(test.text.values, tokenizer, max_len=150)\n",
        "train_labels = train.target.values"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aHFt7Ovhivd",
        "colab_type": "text"
      },
      "source": [
        "## 4.1 Train Base Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JxsxpYqWhuB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c99d7447-e315-4b2c-b64f-95d20a0686a6"
      },
      "source": [
        "train_history = model.fit(\n",
        "    [train_input, train_meta_input], train_labels, \n",
        "    validation_split=0.2,\n",
        "    epochs=4,\n",
        "    callbacks=[checkpoint],\n",
        "    batch_size=32\n",
        ")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "187/187 [==============================] - 116s 618ms/step - loss: 0.5452 - accuracy: 0.7383 - val_loss: 0.4130 - val_accuracy: 0.8270\n",
            "Epoch 2/4\n",
            "187/187 [==============================] - 117s 624ms/step - loss: 0.4063 - accuracy: 0.8439 - val_loss: 0.3765 - val_accuracy: 0.8397\n",
            "Epoch 3/4\n",
            "187/187 [==============================] - 113s 604ms/step - loss: 0.3380 - accuracy: 0.8717 - val_loss: 0.4011 - val_accuracy: 0.8330\n",
            "Epoch 4/4\n",
            "187/187 [==============================] - 114s 607ms/step - loss: 0.2832 - accuracy: 0.9012 - val_loss: 0.4692 - val_accuracy: 0.8310\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5TF59VBhxu6",
        "colab_type": "text"
      },
      "source": [
        "## 4.2 Train Large Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U1IzeuKdmxo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "2e86b571-1b03-4a55-84cc-3622f278059c"
      },
      "source": [
        "train_history = model_large.fit(\n",
        "    [train_input, train_meta_input], train_labels, \n",
        "    validation_split=0.2,\n",
        "    epochs=4,\n",
        "    callbacks=[checkpoint],\n",
        "    batch_size=16\n",
        ")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "373/373 [==============================] - 405s 1s/step - loss: 0.5402 - accuracy: 0.7461 - val_loss: 0.4126 - val_accuracy: 0.8310\n",
            "Epoch 2/4\n",
            "373/373 [==============================] - 407s 1s/step - loss: 0.4012 - accuracy: 0.8364 - val_loss: 0.3877 - val_accuracy: 0.8417\n",
            "Epoch 3/4\n",
            "373/373 [==============================] - 336s 900ms/step - loss: 0.3297 - accuracy: 0.8761 - val_loss: 0.4085 - val_accuracy: 0.8337\n",
            "Epoch 4/4\n",
            "373/373 [==============================] - 339s 909ms/step - loss: 0.2602 - accuracy: 0.9043 - val_loss: 0.4821 - val_accuracy: 0.8109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWgkH1DXJiU9",
        "colab_type": "text"
      },
      "source": [
        "# 5. Herausforderungen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNYacTpZdA6H",
        "colab_type": "text"
      },
      "source": [
        "Das Training zeigt bereits nach drei Epochen Anzeichen für Overfitting. Die Trainings-Accuracy nimmt stark zu, die Validation-Accuracy allerdings nicht. Es ist daher damit zu rechnen, dass sich auch die Ergebnisse des Test-Datensatzes mit weiteren Epochen nicht mehr verbessern oder sogar verschlechtern.\n",
        "\n",
        "Allgemein lagen alle Ergebnisse nah beieinander zwischen 82% und 85%. Das beste Ergebnis des Testsets lag bei 0.84523."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMSjkKNcQlCl",
        "colab_type": "text"
      },
      "source": [
        "Bessere Ergebnisse konnten mit größerer Batchsize erreicht werden, jedoch ist der GPU-Speicher hier limitierend. Die Herausforderung hierbei war es, die Batchsize so groß wie möglich einzustellen, ohne dass das Training mit einem Out-Of-Memory-Error abstürzt.\n",
        "\n",
        "Bei 16GB Memory konnte eine maximale Batchsize von 18 erreicht werden. Dazu musste folgendes Optimiert werden:\n",
        "\n",
        "*   verringerung der max_seq_len von 160 auf 150\n",
        "*   aktivieren von NVidia AMP beim Optimizer\n",
        "*   regelmäßiges Neustarten der Runtime\n",
        "\n",
        "Folgendes könnte noch ausprobiert werden, wurde aber aufgrund des erhöhten Aufwandes und mangels funktionierender Implementationen noch nicht umgesetzt:\n",
        "\n",
        "*   Gradient Accumulation -> Mini-Batches zu größeren Batches zusammenfügen\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1BY-1ttShtw",
        "colab_type": "text"
      },
      "source": [
        "BERT Base vs. BERT Large\n",
        "\n",
        "Mit BERT Large konnten allgemein etwas bessere Ergebnisse erzielt werden. Bei wenig verfügbarem Speicher bietet BERT Base allerdings eine gute Alternative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf2j0gVjWhuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_large.load_weights('model_large.h5')\n",
        "test_pred = model_large.predict([test_input, test_meta_input])\n",
        "test['prediction'] = test_pred"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrownArHbzqd",
        "colab_type": "text"
      },
      "source": [
        "Das folgende Diagramm zeigt, wie die Vorhersagen des Modells verteilt sind. Je größer die beiden Spitzen nahe 0 und 1 sind, desto sicherer entscheidet das Modell. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfDFwsGYv-cQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "3304ad02-db91-4db8-a6c4-bcf4c6792164"
      },
      "source": [
        "sorted = np.sort(test_pred, axis=0)\n",
        "plt.hist(test_pred, bins = 100) \n",
        "plt.title(\"histogram\") \n",
        "plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M8jf1FrVA8o",
        "colab_type": "text"
      },
      "source": [
        "Mit dem folgenden Code sollten die \"unsicheren\" Vorhersagen betrachtet werden um noch Ideen für mögliche Verbesserungen zu finden. Es gab jedoch keine besonderen Auffälligkeiten. Die Tweets werden allgemein recht eindeutig zugeordnet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jiUfH0wTUgE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "8b8b6953-d14f-42ed-c586-b9484912a437"
      },
      "source": [
        "print(test.shape)\n",
        "uncertain = test[(test['prediction'] < 0.80)]\n",
        "print(uncertain.shape)\n",
        "uncertain = uncertain[(uncertain['prediction'] > 0.5)]\n",
        "print(uncertain.shape)\n",
        "uncertain.sort_values(by='keyword_target_mean', ascending=False).head(10)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3263, 12)\n",
            "(2352, 12)\n",
            "(279, 12)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>word_count</th>\n",
              "      <th>unique_word_count</th>\n",
              "      <th>url_count</th>\n",
              "      <th>mean_word_length</th>\n",
              "      <th>char_count</th>\n",
              "      <th>hashtag_count</th>\n",
              "      <th>mention_count</th>\n",
              "      <th>keyword_target_mean</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7508</th>\n",
              "      <td>oil%20spill</td>\n",
              "      <td>missing_location</td>\n",
              "      <td>Got the gas and the splash like an oil spill.</td>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>3.600000</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.546539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7258</th>\n",
              "      <td>nuclear%20disaster</td>\n",
              "      <td>Japan</td>\n",
              "      <td>Less consuming less energy to avoid nuclear di...</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>5.363636</td>\n",
              "      <td>69</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.911765</td>\n",
              "      <td>0.502578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8185</th>\n",
              "      <td>rescuers</td>\n",
              "      <td>Florida</td>\n",
              "      <td>When Rescuers Found Him He ... http://t.co/sTD...</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6.714286</td>\n",
              "      <td>53</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>0.712368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4761</th>\n",
              "      <td>evacuated</td>\n",
              "      <td>Arizona, USA</td>\n",
              "      <td>@misschaela_ not yet.  Everywhere else except ...</td>\n",
              "      <td>24</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>4.583333</td>\n",
              "      <td>134</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.885714</td>\n",
              "      <td>0.534595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4804</th>\n",
              "      <td>evacuated</td>\n",
              "      <td>Everywhere..</td>\n",
              "      <td>Why does the secret bunker used by Cheney duri...</td>\n",
              "      <td>21</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>4.523810</td>\n",
              "      <td>115</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.885714</td>\n",
              "      <td>0.620490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10465</th>\n",
              "      <td>wild%20fires</td>\n",
              "      <td>missing_location</td>\n",
              "      <td>Go home California you're drunk. Natural selec...</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>5.227273</td>\n",
              "      <td>136</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.569835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10460</th>\n",
              "      <td>wild%20fires</td>\n",
              "      <td>Where e'er the mood takes me</td>\n",
              "      <td>@MrRat395 Are those wild fires getting anywher...</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>5.444444</td>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.694754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10459</th>\n",
              "      <td>wild%20fires</td>\n",
              "      <td>North Tonawanda, NY</td>\n",
              "      <td>California wild fires blow my mind every time</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>4.750000</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.697542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10453</th>\n",
              "      <td>wild%20fires</td>\n",
              "      <td>New york</td>\n",
              "      <td>Hey @Macys ! My moms house burned down in the ...</td>\n",
              "      <td>27</td>\n",
              "      <td>26</td>\n",
              "      <td>1</td>\n",
              "      <td>4.296296</td>\n",
              "      <td>142</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.638424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6894</th>\n",
              "      <td>mass%20murder</td>\n",
              "      <td>missing_location</td>\n",
              "      <td>Amazing how smug this guy is about the use of ...</td>\n",
              "      <td>18</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>4.888889</td>\n",
              "      <td>105</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.843750</td>\n",
              "      <td>0.701953</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  keyword  ... prediction\n",
              "id                         ...           \n",
              "7508          oil%20spill  ...   0.546539\n",
              "7258   nuclear%20disaster  ...   0.502578\n",
              "8185             rescuers  ...   0.712368\n",
              "4761            evacuated  ...   0.534595\n",
              "4804            evacuated  ...   0.620490\n",
              "10465        wild%20fires  ...   0.569835\n",
              "10460        wild%20fires  ...   0.694754\n",
              "10459        wild%20fires  ...   0.697542\n",
              "10453        wild%20fires  ...   0.638424\n",
              "6894        mass%20murder  ...   0.701953\n",
              "\n",
              "[10 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9Ta5kzdxojT",
        "colab_type": "text"
      },
      "source": [
        "# 6. Final Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TqHBrH2bDyJ",
        "colab_type": "text"
      },
      "source": [
        "Abschließend werden die Vorhersagen, im geforderten Format in der submission.csv gespeichert. Diese kann dann heruntergeladen und bei Kaggle eingereicht werden. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoUs_gRPJw_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission['target'] = test_pred.round().astype(int)\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irQfKRozjRPc",
        "colab_type": "text"
      },
      "source": [
        "## 6.1 Einordung Kaggle-Ergebnisse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpg4-GFNjhsR",
        "colab_type": "text"
      },
      "source": [
        "*   Kaggle User ID 5149786\n",
        "\n",
        "Das beste Ergebnis des Testsets liegt mit 0.84523 aktuell (09.08.2020) auf Platz 95 bei 1532 Teilnehmern. Die Plätze 1 bis 73 haben jedoch unrealistisch hohe Ergebnisse (meist 1.00000), da der Testdatensatz inkulsive korrekter Vorhersagen öffentlich zugänglich ist. Damit könnte man natürlich auch trainieren, sodass Overfitting kein Problem und sogar erwünscht wäre. Das widerspricht allerdings dann dem Sinn der Aufgabe. Alternativ könnte man auch einfach die korrekten Ergebnisse einreichen, um einen perfekten Score zu erzielen. Das echte Ergebnis von 0.84523 liegt somit eigentlich in den Top-25 der realistischen Ergebnissen, welche bis maximal 0.86607 gehen. Kleine Verbesserungen könnten mit einer oder mehreren GPUs erreicht werden, die mehr als 16GB Speicher haben. Im Original-Paper von Google zu BERT wurde eine TPU mit 64GB verwendet. Dies würde größere Batchsizes erlauben, was zu etwas besseren Ergebnissen führen könnte. Allgemein lässt sich aber festhalten, dass das Modell, welches in diesem Notebook genutzt wurde sehr gute Ergebnisse liefern kann."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC-SqaJtjhbO",
        "colab_type": "text"
      },
      "source": [
        "## 6.2 Modell speichern\n",
        "Außerdem kann das Modell in Google Drive gespeichtert werden."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDifqsiz5qX5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c17bfb22-915a-4730-d1a0-99b556896d63"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root_path = '/content/drive/My Drive/INFM/DataScience/Project'"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrFFzQ-P6hgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_large.save(root_path + '/models/model_0908.h5') "
      ],
      "execution_count": 27,
      "outputs": []
    }
  ]
}