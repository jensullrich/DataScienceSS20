{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Kopie von Kopie von Project_NLP_disaster_tweets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jensullrich/DataScienceSS20/blob/Project/Project/Project_NLP_disaster_tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3VqmBdVjTha",
        "colab_type": "text"
      },
      "source": [
        "# 1. Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF3ebFyhjl-v",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilDCD8aJR0oD",
        "colab_type": "text"
      },
      "source": [
        "Zuerst werden einige Pakete installiert, die in Colab nicht standardmäßig vorhanden sind. Alles weitere sollte vorinstalliert sein. \n",
        "\n",
        "Als Framework wird Tensorflow in der Version 2.3 verwendet. \n",
        "\n",
        "Als Architektur wird die, in der Vorlesung vorgestellte, Architektur BERT verwendet. Es wird die fertige Implementierung bert-for-tf2 in der aktuellen Version 0.14.5 genutzt. \n",
        "Quelle: https://pypi.org/project/bert-for-tf2/\n",
        "\n",
        "Vortrainierte Modelle werden von Tensorflow-Hub heruntergeladen. \n",
        "Quelle: https://tfhub.dev/s?q=bert\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhWwrTl9WhtZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "29c6f422-65fa-42e3-e3b3-987fdcd34413"
      },
      "source": [
        "!pip install tensorflow\n",
        "!pip install --upgrade tensorflow-hub\n",
        "!pip install bert-for-tf2\n",
        "#!pip install sentencepiece"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.30.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (49.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
            "Requirement already up-to-date: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (49.2.0)\n",
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.14.5)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.9.7)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EDLMQIpjroy",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca78jJXhUWhI",
        "colab_type": "text"
      },
      "source": [
        "Hier befinden sich gesammelt alle Imports des Projekts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXBtB1X_Whtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input, Concatenate, Dropout, Average\n",
        "from tensorflow.keras.optimizers import Optimizer, Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow_hub as hub\n",
        "import bert\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLyTrMubj0jZ",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRzua5M0UeGF",
        "colab_type": "text"
      },
      "source": [
        "Das Projekt ist in Google Colab entwickelt und getestet worden, sollte aber auch lokal funktionieren, wenn eine passende GPU vorhanden ist."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQx9fefhW1DH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a344f02b-33bf-4361-ce5a-4d485bbd1de3"
      },
      "source": [
        "#check if notebook runs in colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print('running in Colab:',IN_COLAB)\n",
        "path='..'\n",
        "if IN_COLAB:\n",
        "  #in colab, we need to clone the data from the repo\n",
        "  !git clone -b Project https://github.com/jensullrich/DataScienceSS20/\n",
        "  path='DataScienceSS20/Project'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running in Colab: True\n",
            "fatal: destination path 'DataScienceSS20' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogJoYcz5U8Al",
        "colab_type": "text"
      },
      "source": [
        "Da das BERT Modell sehr groß ist, wird sehr viel GPU-Speicher benötigt. Deshalb wird hier am Anfang geprüft ob eine GPU mit ausreichend Speicher zugeteilt wurde. Es sollten 16GB sein, ist das nicht der Fall sollte die Runtime resettet werden, bis eine passende GPU zur Verfügung steht."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFpC-TWQlIhr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "d51cdc86-22e1-4c3d-f52b-cc656ec4db2e"
      },
      "source": [
        "#check out the GPU\n",
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Aug  8 18:00:20 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrwkowLQWhtY",
        "colab_type": "text"
      },
      "source": [
        "# 2. Exploring Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7SK7tc-VgdK",
        "colab_type": "text"
      },
      "source": [
        "Die Daten werden zunächst explorativ untersucht. Das Traininsset besteht aus 7613 Tweets, die mit 0 oder 1 für \"No Disaster\" oder \"Disaster\" gelabelt sind. Zusätzlich gibt es bei einigen Tweets noch die Location und ein Keyword. Diese können aber auch fehlen.\n",
        "\n",
        "Das Testset enthält 3263 Tweets ohne Label. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf7DIuDaWhtf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read the given csv files\n",
        "train=pd.read_csv(path + '/DATA/train.csv', index_col='id')\n",
        "test=pd.read_csv(path + '/DATA/test.csv', index_col='id')\n",
        "submission = pd.read_csv(path + '/DATA/sample_submission.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXV_wFGLWhti",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "8508d624-71ab-4c5e-93eb-8592d94cf0a3"
      },
      "source": [
        "print(train.shape)\n",
        "print(test.shape)\n",
        "\n",
        "print(train.head())\n",
        "print(test.head())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7613, 4)\n",
            "(3263, 3)\n",
            "   keyword location                                               text  target\n",
            "id                                                                            \n",
            "1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1\n",
            "4      NaN      NaN             Forest fire near La Ronge Sask. Canada       1\n",
            "5      NaN      NaN  All residents asked to 'shelter in place' are ...       1\n",
            "6      NaN      NaN  13,000 people receive #wildfires evacuation or...       1\n",
            "7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...       1\n",
            "   keyword location                                               text\n",
            "id                                                                    \n",
            "0      NaN      NaN                 Just happened a terrible car crash\n",
            "2      NaN      NaN  Heard about #earthquake is different cities, s...\n",
            "3      NaN      NaN  there is a forest fire at spot pond, geese are...\n",
            "9      NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
            "11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-LasmpKWhtk",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 NaN Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em3DRgAKmK-l",
        "colab_type": "text"
      },
      "source": [
        "Laut Beschreibung können sowohl Keyword, als auch Location fehlen. Es wird also zunächst untersucht wie viele Werte fehlen. Beim Keyword ist der Anteil an fehlenden Werten sehr gering, was für eine Verwendung spricht. Bei rund einem Drittel der Daten fehlt die Location, diese ist somit eher schlecht als Feature geeignet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CO1zyVqWhtl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2c485cec-5c95-4152-f804-0246133b0d03"
      },
      "source": [
        "training_has_keyword = train[\"keyword\"].notna().value_counts(normalize=True)\n",
        "training_has_location = train[\"location\"].notna().value_counts(normalize=True)\n",
        "test_has_keyword = test[\"keyword\"].notna().value_counts(normalize=True)\n",
        "test_has_location = test[\"location\"].notna().value_counts(normalize=True)\n",
        "\n",
        "try:\n",
        "    print(f'{training_has_keyword[True]*100:.1f}', \"% of training data have keyword,\", f'{(training_has_keyword[False])*100:.1f}', \"% of keywords are missing\")\n",
        "    print(f'{training_has_location[True]*100:.1f}', \"% of training data have location,\", f'{(training_has_location[False])*100:.1f}', \"% of locations are missing\")\n",
        "    print(f'{test_has_keyword[True]*100:.1f}', \"% of test data have keyword,\", f'{(test_has_keyword[False])*100:.1f}', \"% of keywords are missing\")\n",
        "    print(f'{test_has_location[True]*100:.1f}', \"% of test data have location,\", f'{(test_has_location[False])*100:.1f}', \"% of locations are missing\")\n",
        "except KeyError:\n",
        "    print(\"No NaN values or they have already been replaced\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99.2 % of training data have keyword, 0.8 % of keywords are missing\n",
            "66.7 % of training data have location, 33.3 % of locations are missing\n",
            "99.2 % of test data have keyword, 0.8 % of keywords are missing\n",
            "66.1 % of test data have location, 33.9 % of locations are missing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHZ6Ef6Fm2f4",
        "colab_type": "text"
      },
      "source": [
        "Die fehlenden Werte bekommen die aussagekräftigeren Einträge 'missing_keyword' bzw. 'missing_location'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CELx-VjZWhtn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b02e53f-6346-4621-cd7f-a75fb05176b0"
      },
      "source": [
        "print(\"Replacing NaN values with missing_keyword and missing_location\")\n",
        "train[\"keyword\"] = train[\"keyword\"].fillna('missing_keyword')\n",
        "train[\"location\"] = train[\"location\"].fillna('missing_location')\n",
        "test[\"keyword\"] = test[\"keyword\"].fillna('missing_keyword')\n",
        "test[\"location\"] = test[\"location\"].fillna('missing_location')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Replacing NaN values with missing_keyword and missing_location\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZJpW-e0Whtp",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Duplicate Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ze39Vt1Xvuh",
        "colab_type": "text"
      },
      "source": [
        "Im Trainingsdatensatz kommen einige Tweets mehrfach vor. Bei Zeilen die komplett doppelt vorkommen, inklusive Target, Keyword und Location, werden nur die Duplikate gelöscht. Bei Zeilen, wo nur der Text übereinstimmt, werden alle gelöscht, da vor allem ein verschiedenes Target das Modell stören könnte. Insgesamt werden so 158 Zeilen gelöscht.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t60FUDpWhtq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "68e2f5f4-7f30-4572-b18e-dc16141ecae2"
      },
      "source": [
        "#duplicates = train[train.duplicated()]\n",
        "# remove complete duplicates, keep only the first occurence\n",
        "print(train.shape)\n",
        "train.drop_duplicates(keep='first', inplace=True)\n",
        "print(train.shape)\n",
        "# check other duplicates\n",
        "train.drop_duplicates('text', keep=False, inplace=True)\n",
        "print(train.shape)\n",
        "#duplicates = train[train.duplicated(['text'], keep=False)]\n",
        "#duplicates.shape\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7613, 4)\n",
            "(7561, 4)\n",
            "(7455, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GViHqonsWhts",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 Exploration of keyword and location"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyXyBInvZ3s5",
        "colab_type": "text"
      },
      "source": [
        "Als nächstes werden die Zusatzfeatures Keyword und Location nochmals untersucht. Es gibt 221 verschiedene Keywords und 3306 verschiedene Locations. Wie schon bei den fehlenden Werten stellt sich auch hier heraus, dass das Keyword durchaus hilfreich sein könnte um die Tweets zu klassifizieren. Die Location ist eher ungeeignet, da es zu viele verschiedene Werte gibt, jeder Wert nur wenige Male vorkommt und zusätzlich viele Werte fehlen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTIIdk0hWhtt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "aef1c23d-14cc-4d45-ae5d-1f820f719810"
      },
      "source": [
        "print(\"We have\", len(train[\"keyword\"]), \"tweets with\", train[\"keyword\"].nunique(), \"unique keywords.\")\n",
        "print(\"We have\", len(train[\"location\"]), \"tweets with\", train[\"location\"].nunique(), \"unique locations.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 7455 tweets with 222 unique keywords.\n",
            "We have 7455 tweets with 3307 unique locations.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M2Bi9ZuWhtv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "720b0d3d-a57f-4e83-b40d-cce8677bfbf4"
      },
      "source": [
        "grouped = train.groupby(train[\"keyword\"])\n",
        "grouped.size().sort_values()\n",
        "\n",
        "grouped = train.groupby(train[\"location\"])\n",
        "grouped.target.sum().sort_values(ascending=False).head(10)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "location\n",
              "missing_location    1030\n",
              "USA                   67\n",
              "United States         27\n",
              "Mumbai                18\n",
              "Nigeria               17\n",
              "India                 17\n",
              "New York              16\n",
              "London                16\n",
              "Washington, DC        15\n",
              "UK                    15\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzaVzysnoKXS",
        "colab_type": "text"
      },
      "source": [
        "## 2.4 Meta Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RprbhamDazf3",
        "colab_type": "text"
      },
      "source": [
        "Aus dem Text des Tweets lassen sich noch einige Features bestimmen, die eventuell nützlich sein könnten.\n",
        "\n",
        "\n",
        "*   Anzahl der Wörter\n",
        "*   Anzahl der verschiedenen Wörter\n",
        "*   Anzahl an URLs\n",
        "*   Durchschnittliche länge der Wörter\n",
        "*   Anzahl der Buchstaben\n",
        "*   Anzahl der Hashtags\n",
        "*   Anzahl der Erwähnungen mit @\n",
        "*   keyword_target_mean ist der durchschnittliche Targetwert, bei Tweets mit diesem Keyword\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBeJby5fqMmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word_count\n",
        "train['word_count'] = train['text'].apply(lambda x: len(str(x).split()))\n",
        "test['word_count'] = test['text'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# unique_word_count\n",
        "train['unique_word_count'] = train['text'].apply(lambda x: len(set(str(x).split())))\n",
        "test['unique_word_count'] = test['text'].apply(lambda x: len(set(str(x).split())))\n",
        "\n",
        "# url_count\n",
        "train['url_count'] = train['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
        "test['url_count'] = test['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
        "\n",
        "# mean_word_length\n",
        "train['mean_word_length'] = train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "test['mean_word_length'] = test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "\n",
        "# char_count\n",
        "train['char_count'] = train['text'].apply(lambda x: len(str(x)))\n",
        "test['char_count'] = test['text'].apply(lambda x: len(str(x)))\n",
        "\n",
        "# hashtag_count\n",
        "train['hashtag_count'] = train['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
        "test['hashtag_count'] = test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
        "\n",
        "# mention_count\n",
        "train['mention_count'] = train['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
        "test['mention_count'] = test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
        "\n",
        "# keyword target mean\n",
        "train['keyword_target_mean'] = train.groupby('keyword')['target'].transform('mean')\n",
        "test['keyword_target_mean'] = test['keyword'].apply(lambda x: train.loc[train['keyword'] == x, 'keyword_target_mean'].iloc[0])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF9rT9cyoP6V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "f4aa9c84-3a3a-420c-c666-7da4d9caae5a"
      },
      "source": [
        "train.sort_values(by='keyword_target_mean', ascending=True).head(100)\n",
        "#test.sort_values(by='keyword_target_mean', ascending=False).head(10)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>word_count</th>\n",
              "      <th>unique_word_count</th>\n",
              "      <th>url_count</th>\n",
              "      <th>mean_word_length</th>\n",
              "      <th>char_count</th>\n",
              "      <th>hashtag_count</th>\n",
              "      <th>mention_count</th>\n",
              "      <th>keyword_target_mean</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>aftershock</td>\n",
              "      <td>United States</td>\n",
              "      <td>&amp;gt;&amp;gt; $15 Aftershock : Protect Yourself and...</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>7.187500</td>\n",
              "      <td>130</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>aftershock</td>\n",
              "      <td>Instagram - @heyimginog</td>\n",
              "      <td>@afterShock_DeLo scuf ps live and the game... cya</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>5.250000</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>aftershock</td>\n",
              "      <td>304</td>\n",
              "      <td>'The man who can drive himself further once th...</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>4.500000</td>\n",
              "      <td>110</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>aftershock</td>\n",
              "      <td>Switzerland</td>\n",
              "      <td>320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/yN...</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>7.687500</td>\n",
              "      <td>138</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>aftershock</td>\n",
              "      <td>304</td>\n",
              "      <td>'There is no victory at bargain basement price...</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>5.727273</td>\n",
              "      <td>73</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8356</th>\n",
              "      <td>ruin</td>\n",
              "      <td>Garrett</td>\n",
              "      <td>like why on earth would you want anybody to be...</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>101</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.027027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8375</th>\n",
              "      <td>ruin</td>\n",
              "      <td>Winnipeg, Manitoba</td>\n",
              "      <td>Why do u ruin everything?  @9tarbox u ruined t...</td>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>4.190476</td>\n",
              "      <td>109</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.027027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8382</th>\n",
              "      <td>ruin</td>\n",
              "      <td>Florida Forever</td>\n",
              "      <td>RT to ruin @connormidd 's day.  http://t.co/kr...</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6.714286</td>\n",
              "      <td>54</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.027027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8385</th>\n",
              "      <td>ruin</td>\n",
              "      <td>#RedSoxNation</td>\n",
              "      <td>@JulieChen she shouldn't. Being with them is g...</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>4.588235</td>\n",
              "      <td>94</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.027027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8378</th>\n",
              "      <td>ruin</td>\n",
              "      <td>Boston</td>\n",
              "      <td>Lol The real issue is the the way the NFL is t...</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>4.722222</td>\n",
              "      <td>102</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.027027</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         keyword                  location  ... mention_count  keyword_target_mean\n",
              "id                                          ...                                   \n",
              "178   aftershock             United States  ...             1             0.000000\n",
              "146   aftershock  Instagram - @heyimginog   ...             1             0.000000\n",
              "149   aftershock                       304  ...             0             0.000000\n",
              "151   aftershock               Switzerland  ...             1             0.000000\n",
              "153   aftershock                       304  ...             0             0.000000\n",
              "...          ...                       ...  ...           ...                  ...\n",
              "8356        ruin                   Garrett  ...             0             0.027027\n",
              "8375        ruin        Winnipeg, Manitoba  ...             1             0.027027\n",
              "8382        ruin           Florida Forever  ...             1             0.027027\n",
              "8385        ruin             #RedSoxNation  ...             1             0.027027\n",
              "8378        ruin                    Boston  ...             0             0.027027\n",
              "\n",
              "[100 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m6Dc-L7h4Fk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8333c19f-3563-4d6b-c7ba-6dd845c517fd"
      },
      "source": [
        "train['char_count'].max()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "157"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPRhgrTmWhtx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4fca83ab-999d-4631-c05c-c065ac8eca9b"
      },
      "source": [
        "#define and scale our added features\n",
        "train_meta_input = StandardScaler().fit_transform(train.iloc[:, 4:])\n",
        "test_meta_input = StandardScaler().fit_transform(test.iloc[:, 3:])\n",
        "\n",
        "META_DIM = train_meta_input.shape[1]\n",
        "print(META_DIM)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf7mcleFWhtz",
        "colab_type": "text"
      },
      "source": [
        "# 3. Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5N0HPbIWht2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bert_encode(tweet_texts, tokenizer, max_len=512):\n",
        "    all_tokens = []\n",
        "    all_masks = []\n",
        "    all_segments = []\n",
        "\n",
        "    for text in tweet_texts:\n",
        "        text = tokenizer.tokenize(text)\n",
        "            \n",
        "        text = text[:max_len-2]\n",
        "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "        pad_len = max_len - len(input_sequence)\n",
        "        \n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
        "        tokens += [0] * pad_len\n",
        "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "        segment_ids = [0] * max_len\n",
        "        \n",
        "        all_tokens.append(tokens)\n",
        "        all_masks.append(pad_masks)\n",
        "        all_segments.append(segment_ids)\n",
        "    \n",
        "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkGvFVzfWht4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(bert_layer, max_len=512):\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "    meta_input = Input(shape=(META_DIM,), dtype=tf.float32, name='meta_input')\n",
        "\n",
        "    pool_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "    x = Dense(32,activation='relu')(pool_output)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Concatenate()([x, meta_input])\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    out = Dense(1, activation='sigmoid')(x)\n",
        "    \n",
        "    model = Model(inputs=[input_word_ids, input_mask, segment_ids, meta_input], outputs=out)\n",
        "    opt = tf.keras.optimizers.Adam(lr=1e-5)\n",
        "    opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\n",
        "    model.compile(opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    #model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSgxmRPFcrKC",
        "colab_type": "text"
      },
      "source": [
        "# 3.1 Build Base Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_mY6f-oWht6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "38246231-5372-4c42-a83e-983ce2749b48"
      },
      "source": [
        "%%time\n",
        "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\"\n",
        "bert_layer = hub.KerasLayer(module_url, trainable=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4.58 s, sys: 952 ms, total: 5.53 s\n",
            "Wall time: 5.45 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eD7fFqLWht8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "516452cc-7144-4df4-f61e-006c2bf28b9e"
      },
      "source": [
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "FullTokenizer=bert.bert_tokenization.FullTokenizer\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-7b567224ad76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolved_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masset_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdo_lower_case\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolved_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mFullTokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_tokenization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFullTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFullTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'bert_layer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E8IzUWQhh8n",
        "colab_type": "text"
      },
      "source": [
        "Die Sequence-Length sollte so klein wie möglich und so groß wie nötig sein, da eine größere Länge den Speicherbedarf erhöht."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AXL5eTDWht-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_input = bert_encode(train.text.values, tokenizer, max_len=160)\n",
        "test_input = bert_encode(test.text.values, tokenizer, max_len=160)\n",
        "train_labels = train.target.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pz0tDqIcWht_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(bert_layer, max_len=160)\n",
        "model.summary()\n",
        "\n",
        "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uXaQe_5cktZ",
        "colab_type": "text"
      },
      "source": [
        "# 3.2 Build Large Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCxBp96Mc3CH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1da95b62-729c-4959-cea4-783177151a33"
      },
      "source": [
        "%%time\n",
        "module_large_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/2\"\n",
        "bert_large_layer = hub.KerasLayer(module_large_url, trainable=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 8.74 s, sys: 2.14 s, total: 10.9 s\n",
            "Wall time: 10.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDGD_49bdC4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_file = bert_large_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_large_layer.resolved_object.do_lower_case.numpy()\n",
        "FullTokenizer=bert.bert_tokenization.FullTokenizer\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TaDMUm3dXLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_input = bert_encode(train.text.values, tokenizer, max_len=150)\n",
        "test_input = bert_encode(test.text.values, tokenizer, max_len=150)\n",
        "train_labels = train.target.values"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du5IjxoldZlp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f6f79e5c-5daf-4766-ef8c-cdfaf45875a6"
      },
      "source": [
        "model_large = build_model(bert_large_layer, max_len=150)\n",
        "model_large.summary()\n",
        "\n",
        "checkpoint = ModelCheckpoint('model_large.h5', monitor='val_loss', save_best_only=True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 150)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 150)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 150)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 32)           32800       keras_layer[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 32)           0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "meta_input (InputLayer)         [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 40)           0           dropout[0][0]                    \n",
            "                                                                 meta_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 40)           0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 40)           0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 40)           0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 40)           0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 40)           0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 40)           0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 40)           0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 40)           0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 40)           0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 64)           2624        dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 64)           2624        dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 64)           2624        dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 64)           2624        dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 64)           2624        dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 64)           2624        dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 64)           2624        dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 64)           2624        dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "sample_0 (Dense)                (None, 1)            65          dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sample_1 (Dense)                (None, 1)            65          dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sample_2 (Dense)                (None, 1)            65          dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "sample_3 (Dense)                (None, 1)            65          dense_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "sample_4 (Dense)                (None, 1)            65          dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "sample_5 (Dense)                (None, 1)            65          dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "sample_6 (Dense)                (None, 1)            65          dense_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "sample_7 (Dense)                (None, 1)            65          dense_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "output (Average)                (None, 1)            0           sample_0[0][0]                   \n",
            "                                                                 sample_1[0][0]                   \n",
            "                                                                 sample_2[0][0]                   \n",
            "                                                                 sample_3[0][0]                   \n",
            "                                                                 sample_4[0][0]                   \n",
            "                                                                 sample_5[0][0]                   \n",
            "                                                                 sample_6[0][0]                   \n",
            "                                                                 sample_7[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 335,196,201\n",
            "Trainable params: 335,196,200\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP_hxt4Yxc2u",
        "colab_type": "text"
      },
      "source": [
        "# 4. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JxsxpYqWhuB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "113914a5-e5ed-4dce-843e-458af01c528e"
      },
      "source": [
        "train_history = model.fit(\n",
        "    [train_input, train_meta_input], train_labels, \n",
        "    validation_split=0.2,\n",
        "    epochs=4,\n",
        "    callbacks=[checkpoint],\n",
        "    batch_size=32\n",
        ")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-569a2484506a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_history = model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_meta_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U1IzeuKdmxo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "fcd16742-2126-44cc-b7ff-6c16ba4bb58d"
      },
      "source": [
        "train_history = model_large.fit(\n",
        "    [train_input, train_meta_input], train_labels, \n",
        "    validation_split=0.2,\n",
        "    epochs=4,\n",
        "    callbacks=[checkpoint],\n",
        "    batch_size=18\n",
        ")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "332/332 [==============================] - 454s 1s/step - loss: 0.4132 - accuracy: 0.8732 - val_loss: 0.4174 - val_accuracy: 0.8364\n",
            "Epoch 2/2\n",
            "332/332 [==============================] - 451s 1s/step - loss: 0.3909 - accuracy: 0.8843 - val_loss: 0.4172 - val_accuracy: 0.8357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWgkH1DXJiU9",
        "colab_type": "text"
      },
      "source": [
        "# 5. Herausforderungen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNYacTpZdA6H",
        "colab_type": "text"
      },
      "source": [
        "Das Training zeigt bereits nach drei Epochen Anzeichen für Overfitting. Die Trainings-Accuracy nimmt stark zu, die Validation-Accuracy allerdings nicht. Es ist daher damit zu rechnen, dass sich auch die Ergebnisse des Test-Datensatzes mit weiteren Epochen nicht mehr verbessern oder sogar verschlechtern.\n",
        "\n",
        "Allgemein lagen alle Ergebnisse nah beieinander zwischen 82% und 85%. Das beste Ergebnis des Testsets lag bei 0.84523."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMSjkKNcQlCl",
        "colab_type": "text"
      },
      "source": [
        "Bessere Ergebnisse konnten mit größerer Batchsize erreicht werden, jedoch ist der GPU-Speicher hier limitierend. Die Herausforderung hierbei war es, die Batchsize so groß wie möglich einzustellen, ohne dass das Training mit einem Out-Of-Memory-Error abstürzt.\n",
        "\n",
        "Bei 16GB Memory konnte eine maximale Batchsize von 18 erreicht werden. Dazu musste folgendes Optimiert werden:\n",
        "\n",
        "*   verringerung der max_seq_len von 160 auf 150\n",
        "*   aktivieren von NVidia AMP beim Optimizer\n",
        "*   regelmäßiges Neustarten der Runtime\n",
        "\n",
        "Folgendes könnte noch ausprobiert werden, wurde aber aufgrund des erhöhten Aufwandes und mangels funktionierender Implementationen noch nicht umgesetzt:\n",
        "\n",
        "*   Gradient Accumulation -> Mini-Batches zu größeren Batches zusammenfügen\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1BY-1ttShtw",
        "colab_type": "text"
      },
      "source": [
        "BERT Base vs. BERT Large\n",
        "\n",
        "Mit BERT Large konnten allgemein etwas bessere Ergebnisse erzielt werden. Bei wenig verfügbarem Speicher bietet BERT Base allerdings eine gute Alternative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf2j0gVjWhuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_large.load_weights('model_large.h5')\n",
        "test_pred = model_large.predict([test_input, test_meta_input])\n",
        "test['prediction'] = test_pred"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrownArHbzqd",
        "colab_type": "text"
      },
      "source": [
        "Das folgende Diagramm zeigt, wie die Vorhersagen des Modells verteilt sind. Je größer die beiden Spitzen nahe 0 und 1 sind, desto sicherer entscheidet das Modell. \n",
        "\n",
        "Es scheint, dass es für das Modell einfacher ist, einen Tweet als \"real Disaster\" zu erkennen, als umgekehrt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfDFwsGYv-cQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "8dc78204-9adc-419e-f15b-b490b0679081"
      },
      "source": [
        "sorted = np.sort(test_pred, axis=0)\n",
        "plt.hist(test_pred, bins = 100) \n",
        "plt.title(\"histogram\") \n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M8jf1FrVA8o",
        "colab_type": "text"
      },
      "source": [
        "Mit dem folgenden Code sollten die \"unsicheren\" Vorhersagen betrachtet werden um noch Ideen für mögliche Verbesserungen zu finden. Es gab jedoch keine besonderen Auffälligkeiten."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jiUfH0wTUgE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "9621d278-01d6-43df-8c09-3ecf39f9ea4f"
      },
      "source": [
        "print(test.shape)\n",
        "uncertain = test[(test['prediction'] < 0.80)]\n",
        "print(uncertain.shape)\n",
        "uncertain = uncertain[(uncertain['prediction'] > 0.5)]\n",
        "print(uncertain.shape)\n",
        "uncertain.sort_values(by='keyword_target_mean', ascending=False).head(10)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3263, 12)\n",
            "(2873, 12)\n",
            "(960, 12)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>word_count</th>\n",
              "      <th>unique_word_count</th>\n",
              "      <th>url_count</th>\n",
              "      <th>mean_word_length</th>\n",
              "      <th>char_count</th>\n",
              "      <th>hashtag_count</th>\n",
              "      <th>mention_count</th>\n",
              "      <th>keyword_target_mean</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10761</th>\n",
              "      <td>wreckage</td>\n",
              "      <td>Mumbai</td>\n",
              "      <td>Wreckage 'Conclusively Confirmed' as From MH37...</td>\n",
              "      <td>17</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>7.058824</td>\n",
              "      <td>136</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.799649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3113</th>\n",
              "      <td>debris</td>\n",
              "      <td>Rochester Minnesota</td>\n",
              "      <td>BBC News - MH370: Reunion debris is from missi...</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>6.333333</td>\n",
              "      <td>87</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.799188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3143</th>\n",
              "      <td>debris</td>\n",
              "      <td>missing_location</td>\n",
              "      <td>With authorities ???increasingly confident??? ...</td>\n",
              "      <td>18</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>6.611111</td>\n",
              "      <td>136</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.798050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3148</th>\n",
              "      <td>debris</td>\n",
              "      <td>World</td>\n",
              "      <td>Top story: MH370: Reunion debris is from missi...</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>2</td>\n",
              "      <td>6.562500</td>\n",
              "      <td>120</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.798070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3151</th>\n",
              "      <td>debris</td>\n",
              "      <td>Bristol, UK</td>\n",
              "      <td>Interesting MH370: Aircraft debris found on La...</td>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>5.850000</td>\n",
              "      <td>136</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.795278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3515</th>\n",
              "      <td>derailment</td>\n",
              "      <td>missing_location</td>\n",
              "      <td>Madhya Pradesh Train Derailment: Village Youth...</td>\n",
              "      <td>21</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>5.523810</td>\n",
              "      <td>136</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.781450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3538</th>\n",
              "      <td>derailment</td>\n",
              "      <td>Chicago</td>\n",
              "      <td>Great photo by the Tribune's Terrence Antonio ...</td>\n",
              "      <td>17</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>6.588235</td>\n",
              "      <td>128</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.799127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3545</th>\n",
              "      <td>derailment</td>\n",
              "      <td>missing_location</td>\n",
              "      <td>Green Line trains resume service after South S...</td>\n",
              "      <td>19</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>6.157895</td>\n",
              "      <td>135</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.796442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3546</th>\n",
              "      <td>derailment</td>\n",
              "      <td>missing_location</td>\n",
              "      <td>Madhya Pradesh Train Derailment: Village Youth...</td>\n",
              "      <td>21</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>5.523810</td>\n",
              "      <td>136</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.781450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10742</th>\n",
              "      <td>wreckage</td>\n",
              "      <td>missing_location</td>\n",
              "      <td>Wreckage 'conclusively confirmed' as from MH37...</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>109</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.797943</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          keyword             location  ... keyword_target_mean  prediction\n",
              "id                                      ...                                \n",
              "10761    wreckage               Mumbai  ...                 1.0    0.799649\n",
              "3113       debris  Rochester Minnesota  ...                 1.0    0.799188\n",
              "3143       debris     missing_location  ...                 1.0    0.798050\n",
              "3148       debris                World  ...                 1.0    0.798070\n",
              "3151       debris          Bristol, UK  ...                 1.0    0.795278\n",
              "3515   derailment     missing_location  ...                 1.0    0.781450\n",
              "3538   derailment              Chicago  ...                 1.0    0.799127\n",
              "3545   derailment     missing_location  ...                 1.0    0.796442\n",
              "3546   derailment     missing_location  ...                 1.0    0.781450\n",
              "10742    wreckage     missing_location  ...                 1.0    0.797943\n",
              "\n",
              "[10 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9Ta5kzdxojT",
        "colab_type": "text"
      },
      "source": [
        "# 6. Final Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TqHBrH2bDyJ",
        "colab_type": "text"
      },
      "source": [
        "Abschließend werden die Vorhersagen, im geforderten Format in der submission.csv gespeichert. Diese kann dann heruntergeladen und bei Kaggle eingereicht werden. \n",
        "\n",
        "Außerdem kann das Modell in Google Drive gespeichtert werden.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoUs_gRPJw_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission['target'] = test_pred.round().astype(int)\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDifqsiz5qX5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c17bfb22-915a-4730-d1a0-99b556896d63"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root_path = '/content/drive/My Drive/INFM/DataScience/Project'"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrFFzQ-P6hgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_large.save(root_path + '/models/model_0808_sub12_best.h5') "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5EA792dWhuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0JxmfruWhuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "338RC7JNWhuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}